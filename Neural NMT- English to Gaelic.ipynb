{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCr4xKJnhZ6"
   },
   "source": [
    "# Overview\n",
    "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
    "\n",
    "**Grading Policy** \n",
    "Assignment 2 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below.  Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. Feel free to reach out to the TAs and instructors if you have any questions.\n",
    "\n",
    "## Task 1 - Data Collection and Preprocessing (10 points)\n",
    "## Task 1a. Data Loading (5 pts)\n",
    "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
    "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
    "* Randomly sample 12,000 rows.\n",
    "* Split the sampled data into train (10k), development (1k) and test set (1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mjieQgrsocnh"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "#creating neccesaary imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# reading in files\n",
    "enf = []\n",
    "gaf = []\n",
    "\n",
    "\n",
    "with open(\"DGT.en-ga.en\", encoding='utf-8') as en:\n",
    "    #enf = en.read().splitlines()\n",
    "    for i in en:\n",
    "        enf.append(en.readline())\n",
    "     \n",
    "with open(\"DGT.en-ga.ga\", encoding='utf-8') as ga:\n",
    "    #gaf = ga.read().splitlines()\n",
    "    for i in ga:\n",
    "        gaf.append(ga.readline())\n",
    "\n",
    " \n",
    "#print(len(enf))\n",
    "#print(len(enf))\n",
    "\n",
    "#checking if data is loaded correctly\n",
    "assert len(enf) == len(gaf)\n",
    "\n",
    "\n",
    "    \n",
    "#reference->https://stackoverflow.com/questions/43152368/python-unicodedecodeerror-charmap-codec-cant-decode-byte-0x81-in-position\n",
    "#reference -> https://www.w3schools.com/python/python_file_open.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe object\n",
    "df = pd.DataFrame({\"en\":enf, \"ga\":gaf})\n",
    "\n",
    "#random sampling 12,000 rows\n",
    "sample_df = df.sample(n = 12000, replace= False, random_state = 42)\n",
    "\n",
    "# performing splits according to : Split the sampled data into train (10k), development (1k) and test set (1k)\n",
    "\n",
    "train = sample_df.head(10000)\n",
    "dev = sample_df.iloc[10000:11000,:]\n",
    "test = sample_df.iloc[11000:12000,:]\n",
    "\n",
    "#validating the shapes\n",
    "assert len(train) == 10000\n",
    "assert len(dev) == 1000\n",
    "assert len(test) == 1000\n",
    "\n",
    "\n",
    "# reference -> https://datatofish.com/random-rows-pandas-dataframe/\n",
    "# reference -> https://sparkbyexamples.com/pandas/get-first-n-rows-of-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejav7LUqokNc"
   },
   "source": [
    "## Task 1b. Preprocessing (5 pts)\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the following pre-processing steps:\n",
    "  * Lowercase the text\n",
    "  * Remove all punctuation\n",
    "  * tokenize the text \n",
    "*  Build seperate vocabularies for each language. \n",
    "  * Assign each unique word an id value \n",
    "*Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CGC-CvmHojdB"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class prep:\n",
    "    def __init__(self, lang: str):\n",
    "        self.lang = lang\n",
    "        self.word2index = {\"PAD\": 0, \"BOF\":1, \"EOS\":2}\n",
    "        self.index2word = {0:\"PAD\", 1:\"BOF\", 2:\"EOS\"}\n",
    "        self.word2count = {}\n",
    "        self.max_len_seq = 0\n",
    "        self.n_words = len(self.index2word)\n",
    "        \n",
    "    \n",
    "    def clean ( self, sentence: str):\n",
    "        \"\"\"\n",
    "        lowercase\n",
    "        remove punct\n",
    "        tokenize\n",
    "        \"\"\"\n",
    "        text = sentence.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "        if len(word_tokenize(clean_text)) > self.max_len_seq:\n",
    "            self.max_len_seq = len(word_tokenize(clean_text))\n",
    "            \n",
    "        for word in word_tokenize(clean_text):\n",
    "              self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word: str):\n",
    "   \n",
    "        if word not in self.word2index:\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "        else:\n",
    "                self.word2count[word] += 1\n",
    "                \n",
    "    def encodeSentence(self, sentence: str) -> List[int]:\n",
    "   \n",
    "        text = sentence.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '',text).strip()\n",
    "        clean_text = \"BOF \" + clean_text + \" EOS\"\n",
    "        return [self.word2index[word] for word in word_tokenize(clean_text) if word in self.word2index]\n",
    "\n",
    "    def decodeIds(self, ids: list) -> List[str]:\n",
    "    \n",
    "        return \" \".join([self.index2word[tok] for tok in ids])\n",
    "    \n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3744878c2284105a5e53bb408c76828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in english language which is equal to unique token  : 11666\n",
      "Total number of samples in gaelic language which is equal to unique token: 16258\n",
      "Max sequence length for English language 175\n",
      "Max sequence length for Gaelic language 268\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "english = prep(\"english\")\n",
    "gaelic = prep(\"gaelic\")\n",
    "\n",
    "for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "  english.clean(row[\"en\"])\n",
    "  gaelic.clean(row[\"ga\"])\n",
    "\n",
    "print(f\"Total number of samples in english language which is equal to unique token  : {english.n_words}\")\n",
    "print(f\"Total number of samples in gaelic language which is equal to unique token: {gaelic.n_words}\")\n",
    "print(\"Max sequence length for English language\", english.max_len_seq)\n",
    "print(\"Max sequence length for Gaelic language\", gaelic.max_len_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7gvR8hz0tMoG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train source (10000, 10), and target (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import torch \n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "def encode_features(\n",
    "    df: pd.DataFrame, \n",
    "    english: prep,\n",
    "    gaelic: prep,\n",
    "    pad_token: int = 0,\n",
    "    max_seq_length = 10\n",
    "  ):\n",
    "\n",
    "  source = []\n",
    "  target = []\n",
    "\n",
    "  for _, row in df.iterrows():\n",
    "    source.append(english.encodeSentence(row[\"en\"]))\n",
    "    target.append(gaelic.encodeSentence(row[\"ga\"]))\n",
    "\n",
    "  source = pad_sequences(\n",
    "      source,\n",
    "      maxlen=max_seq_length,\n",
    "      padding=\"post\",\n",
    "      truncating = \"post\",\n",
    "      value=pad_token\n",
    "    )\n",
    "\n",
    "  target = pad_sequences(\n",
    "      target,\n",
    "      maxlen=max_seq_length,\n",
    "      padding=\"post\",\n",
    "      truncating = \"post\",\n",
    "      value=pad_token\n",
    "    )\n",
    "  \n",
    "  return source, target\n",
    "\n",
    "train_source, train_target = encode_features(train, english,gaelic)\n",
    "val_source, val_target = encode_features(dev, english, gaelic)\n",
    "test_source, test_target = encode_features(test, english, gaelic)\n",
    "\n",
    "print(f\"Shapes of train source {train_source.shape}, and target {train_target.shape}\")\n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oauhQ1fjsC69"
   },
   "source": [
    "## Task 2. Model Implementation and Training (30 pts)\n",
    "\n",
    "\n",
    "\n",
    "## Task 2a. Encoder-Decoder Model Implementation (10 pts)\n",
    "Implement an Encoder-Decoder model in Pytorch with the following components\n",
    "* A single layer RNN based encoder. \n",
    "* A single layer RNN based decoder\n",
    "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(train_source),\n",
    "        torch.LongTensor(train_target)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(val_source),\n",
    "        torch.LongTensor(val_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(test_source),\n",
    "        torch.LongTensor(test_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "#reference-> https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "#reference-> https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "#reference-> https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqdYhxa1uiqF"
   },
   "source": [
    "## Task 2b. Training (10 pts)\n",
    "Implement the code to train the Encoder-Decoder model on the Irish-English data. You will write code for the following:\n",
    "* Training, validation and test dataloaders \n",
    "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = english.n_words\n",
    "OUTPUT_DIM = gaelic.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11666, 256)\n",
       "    (rnn): LSTM(256, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(16258, 256)\n",
       "    (rnn): LSTM(256, 512, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=16258, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa0ca1370f94a139ca0d6373fd76621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4927fcf62f694f77846065d48ea25d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 5.982 | train ppl 396.2320402998885 | val ppl 314.8196704067165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da527b8c9efb45f19082ec68e4ec0f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1959770c4f0144569cb742b4f2219ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss 5.334 | train ppl 207.26537976093547 | val ppl 263.749555636939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1fd5ae59ce48538b20acdfc925a7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0fc2ddf4943058b14dab1074d0bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train loss 5.027 | train ppl 152.4749011684023 | val ppl 228.14924542400394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed43bd3065a24ab69e6ba8bf0ff1f046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd59f25fc3841d3b062ec85010cdf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train loss 4.734 | train ppl 113.74965217224874 | val ppl 230.90352901934028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edca951950a0467b90db6a35942ef88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6032e6ab4e47a9b2c9ebe5b960c78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train loss 4.57 | train ppl 96.54410977284468 | val ppl 225.87912250203328\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "\n",
    "     src = batch[0].transpose(1, 0).to(device)\n",
    "     trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "\n",
    "     output = model(src, trg)\n",
    "\n",
    "     output_dim = output.shape[-1]\n",
    "     output = output[1:].view(-1, output_dim).to(device)\n",
    "     trg = trg[1:].reshape(-1)\n",
    "     \n",
    "     loss = F.cross_entropy(output, trg)\n",
    "     loss.backward()\n",
    "\n",
    "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "     optimizer.step()\n",
    "     epoch_loss += loss.item()\n",
    "\n",
    "  train_loss = round(epoch_loss / len(train_dl), 3)\n",
    "  \n",
    "  eval_loss = 0\n",
    "  model.eval()\n",
    "  for batch in tqdm(val_dl, total=len(val_dl)):\n",
    "    src = batch[0].transpose(1, 0).to(device)\n",
    "    trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model(src, trg)\n",
    "      \n",
    "      output_dim = output.shape[-1]\n",
    "      output = output[1:].view(-1, output_dim).to(device)\n",
    "      trg = trg[1:].reshape(-1)\n",
    "      \n",
    "      loss = F.cross_entropy(output, trg)\n",
    "      \n",
    "      eval_loss += loss.item()\n",
    "  \n",
    "  val_loss = round(eval_loss / len(val_dl), 3)\n",
    "  print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(val_loss)}\")\n",
    "\n",
    "\n",
    "  if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    torch.save(model.state_dict(), 'best-model.pt')  \n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QofrQ1GAwnDz"
   },
   "source": [
    "# Task 2c. Evaluation on the Test Set (10 pts)\n",
    "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    text: str, \n",
    "    model: EncoderDecoder, \n",
    "    english: prep,\n",
    "    gaelic: prep,\n",
    "    device: str,\n",
    "    max_len: int = 10,\n",
    "  ) -> str:\n",
    "\n",
    "  # Encode english sentence and convert to tensor\n",
    "  input_ids = english.encodeSentence(text)\n",
    "  input_tensor = torch.LongTensor(input_ids).unsqueeze(1).to(device)\n",
    "\n",
    "  # Get encooder hidden states\n",
    "  with torch.no_grad():\n",
    "    encoder_outputs, hidden = model.encoder(input_tensor)\n",
    "\n",
    "  # Build target holder list\n",
    "  trg_indexes = [gaelic.word2index[\"BOF\"]]\n",
    "\n",
    "  # Loop over sequence length of target sentence\n",
    "  for i in range(max_len):\n",
    "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "    \n",
    "    # Decode the encoder outputs with respect to current target word\n",
    "    with torch.no_grad():\n",
    "      output, hidden, cell = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "    \n",
    "    # Retrieve most likely word over target distribution\n",
    "    pred_token = torch.argmax(output).item()\n",
    "    trg_indexes.append(pred_token)\n",
    "\n",
    "    if pred_token == gaelic.word2index[\"EOS\"]:\n",
    "      break\n",
    "\n",
    "  return \"\".join(gaelic.decodeIds(trg_indexes))\n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1053830046501567e-157\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "model.eval()\n",
    "score = []\n",
    "for _,row in test.iterrows():\n",
    "        translated = translate_sentence(row[\"en\"], model, english, gaelic, device)\n",
    "        b_score = nltk.translate.bleu_score.sentence_bleu(translated.split(), row[\"ga\"].split())\n",
    "        score.append(b_score)\n",
    "print(np.mean(score))\n",
    "\n",
    "#reference -> Lab_07, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_brvXpVJxD7e"
   },
   "source": [
    "## Task 3. Improving NMT using Attention (10 pts) \n",
    "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. In a few sentences explains which model is the best for translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that BLEU score of model with attention is higher because attention allows the decoder to selectively focus on the parts of the input sequence that are most relevant to generarting next output token, that is , it then computes an attention vector over the length of the sentence which is used by the model to learn which part of the source sentence is most salient when translating the current token. \n",
    "Note that since the architecture has been changed from LSTM to GRU for attention based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_vocab_size,  # size of source vocabulary  \n",
    "        hidden_dim,        # hidden dimension of embeddings\n",
    "        encoder_hid_dim,   # gru hidden dim\n",
    "        decoder_hid_dim,   # decoder hidden dim \n",
    "        dropout_prob = .5\n",
    "      ):\n",
    "      \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, encoder_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards GRU\n",
    "        #hidden [-1, :, : ] is the last of the backwards GRU\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))        \n",
    "        return outputs, hidden\n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_hid_dim,      # Encoder hidden dimension\n",
    "        dec_hid_dim       # Decoder hidden dimension \n",
    "      ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention output: [batch size, src len]\n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_vocab_size,    # Size of target vocab \n",
    "        hidden_dim,           # hidden size of embedding  \n",
    "        enc_hid_dim, \n",
    "        dec_hid_dim, \n",
    "        dropout\n",
    "      ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = target_vocab_size\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "            (enc_hid_dim * 2) + dec_hid_dim + hidden_dim, \n",
    "            target_vocab_size\n",
    "          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)  # [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)     # [batch size, src len]\n",
    "        a = a.unsqueeze(1)                              # [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)           # [batch size, 1, enc hid dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)               # [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0)\n",
    "    \n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time     \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):     \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs\n",
    "    \n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(11666, 256)\n",
       "    (rnn): GRU(256, 128, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(16258, 256)\n",
       "    (rnn): GRU(512, 128)\n",
       "    (fc_out): Linear(in_features=640, out_features=16258, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = english.n_words\n",
    "OUTPUT_DIM = gaelic.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "model = EncoderDecoder(enc, dec)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e31a97e0d8c43feb52978d5ee2792c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcd7b64b52b4ccfb97fe23d4d7aba2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 6.135 | train ppl 461.73909401890546 | val ppl 313.8766266683865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a461efeb97458b839823f69232b3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31121b96f7c549b8863b8e2d492fa398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss 5.308 | train ppl 201.94593236216255 | val ppl 265.33680997200577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940da10785f54273b7019d221fef8a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68599136eeba43968ef761fd067f9867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train loss 5.014 | train ppl 150.50555593211624 | val ppl 239.60698055027026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d567c6f8fdc84434b8bd659c5635a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53be5c1e8964187b9bb68436f6d683c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train loss 4.828 | train ppl 124.9607889885125 | val ppl 230.21185645987578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114f18493c6d41d290afb4639a955fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b03af3369724ceaa5d64e7ac2dc33a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train loss 4.608 | train ppl 100.28338217150392 | val ppl 212.08772791591244\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "  for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "\n",
    "     src = batch[0].transpose(1, 0).to(device)\n",
    "     trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "\n",
    "     output = model(src, trg)\n",
    "\n",
    "     output_dim = output.shape[-1]\n",
    "     output = output[1:].view(-1, output_dim).to(device)\n",
    "     trg = trg[1:].reshape(-1)\n",
    "     \n",
    "     loss = F.cross_entropy(output, trg)\n",
    "     loss.backward()\n",
    "\n",
    "     torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "     optimizer.step()\n",
    "     epoch_loss += loss.item()\n",
    "\n",
    "  train_loss = round(epoch_loss / len(train_dl), 3)\n",
    "  \n",
    "  eval_loss = 0\n",
    "  model.eval()\n",
    "  for batch in tqdm(val_dl, total=len(val_dl)):\n",
    "    src = batch[0].transpose(1, 0).to(device)\n",
    "    trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = model(src, trg)\n",
    "      \n",
    "      output_dim = output.shape[-1]\n",
    "      output = output[1:].view(-1, output_dim).to(device)\n",
    "      trg = trg[1:].reshape(-1)\n",
    "      \n",
    "      loss = F.cross_entropy(output, trg)\n",
    "      \n",
    "      eval_loss += loss.item()\n",
    "  \n",
    "  val_loss = round(eval_loss / len(val_dl), 3)\n",
    "  print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(val_loss)}\")\n",
    "\n",
    "\n",
    "  if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    torch.save(model.state_dict(), 'best-model.pt')  \n",
    "  \n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    text: str, \n",
    "    model: EncoderDecoder, \n",
    "    english: prep,\n",
    "    gaelic: prep,\n",
    "    device: str,\n",
    "    max_len: int = 10,\n",
    "  ) -> str:\n",
    "\n",
    "  # Encode english sentence and convert to tensor\n",
    "  input_ids = english.encodeSentence(text)\n",
    "  input_tensor = torch.LongTensor(input_ids).unsqueeze(1).to(device)\n",
    "\n",
    "  # Get encooder hidden states\n",
    "  with torch.no_grad():\n",
    "    encoder_outputs, hidden = model.encoder(input_tensor)\n",
    "\n",
    "  # Build target holder list\n",
    "  trg_indexes = [gaelic.word2index[\"BOF\"]]\n",
    "\n",
    "  # Loop over sequence length of target sentence\n",
    "  for i in range(max_len):\n",
    "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "    \n",
    "    # Decode the encoder outputs with respect to current target word\n",
    "    with torch.no_grad():\n",
    "      output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "    \n",
    "    # Retrieve most likely word over target distribution\n",
    "    pred_token = torch.argmax(output).item()\n",
    "    trg_indexes.append(pred_token)\n",
    "\n",
    "    if pred_token == gaelic.word2index[\"EOS\"]:\n",
    "      break\n",
    "\n",
    "  return \"\".join(gaelic.decodeIds(trg_indexes))\n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skrma\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\skrma\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\skrma\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7048101190961166e-158\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "model.eval()\n",
    "score = []\n",
    "for _,row in test.iterrows():\n",
    "        translated = translate_sentence(row[\"en\"], model, english, gaelic, device)\n",
    "        b_score = nltk.translate.bleu_score.sentence_bleu(translated.split(), row[\"ga\"].split())\n",
    "        score.append(b_score)\n",
    "print(np.mean(score))\n",
    "\n",
    "#reference -> Lab_08_Neural_NMT, AdvanceNlp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
